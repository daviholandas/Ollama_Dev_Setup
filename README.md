# ğŸ§  Ollama Devâ€‘First Setup

<p align="center">
  <b>Localâ€‘Only AI Environment for Developers</b><br>
  <sub>Configure, orchestrate, and run specialized Ollama personas for software development â€” fully local, GPUâ€‘optimized, and private.</sub>
</p>


<p align="center">
  <img src="https://img.shields.io/badge/license-MIT-green.svg">
  <img src="https://img.shields.io/badge/platform-Linux%20%7C%20macOS%20%7C%20Windows-blue">
  <img src="https://img.shields.io/badge/Ollama-compatible-orange">
</p>


---

## ğŸš€ Overview

This script (`setup_ollama_local.py`) automatically configures **Ollama** for a complete, localâ€‘first software development experience.  
It applies **global environment variables** (systemd rootâ€‘level) and allows **onâ€‘demand creation** of domainâ€‘specific AI personas.

### âœ… Highlights

- ğŸ§© Personaâ€‘based AI agents (Dev, Arch, Test, Plan, Orchestrator)
- âš™ï¸ Global configuration (root/systemd override or user fallback)
- ğŸ’» Optimized for GPUs (e.g., RTX 4060â€‘Ti, 4070, A2000)
- ğŸ” 100% Local â€” no cloud dependency
- ğŸ§± Extensible â€” easily add your own personas

---

## ğŸ§° Installation & Usage

### ğŸ§¾ 1. Set Global Environment (systemd root mode)

If Ollama runs as a system service (installed via `.deb` or `.rpm`):

```bash
sudo python3 setup_ollama_local.py --global-env --threads 12
sudo systemctl daemon-reload && sudo systemctl restart ollama
```

**What this does:**

- Creates `/etc/systemd/system/ollama.service.d/override.conf`

- Applies optimal environment variables:

  ```
  OLLAMA_NUM_GPU=1
  OLLAMA_GPU_LAYERS=999
  OLLAMA_NUM_THREADS=12
  OLLAMA_MAX_LOADED_MODELS=1
  OLLAMA_KEEP_ALIVE=10m
  CUDA_VISIBLE_DEVICES=0
  OLLAMA_FLASH_ATTENTION=1
  ```

<details>
<summary><b>ğŸ’¡ Without sudo (user fallback)</b></summary>


```bash
python3 setup_ollama_local.py --global-env --threads 12
systemctl --user daemon-reload && systemctl --user restart ollama
```

</details>

---

### ğŸ‘¤ 2. Create Personas On Demand

List available personas:

```bash
python3 setup_ollama_local.py --list
```

Pull and create specific personas (example):

```bash
python3 setup_ollama_local.py --pull --create --persona dev,plan
```

Once created, you can run them instantly:

```bash
ollama run dev-agent
ollama run plan-agent
ollama run orch-agent  # CPU-only
```

---

## ğŸ§© Personas & Their Roles

| Persona               | Base Model                            | Role                          | Why Itâ€™s Useful                                         |
| --------------------- | ------------------------------------- | ----------------------------- | ------------------------------------------------------- |
| ğŸ§‘â€ğŸ’» **dev-agent**      | `qwen2.5-coder:14b-instruct-q5_K_M`   | Code generation & refactoring | Expert in .NET, clean code, SOLID, and optimization     |
| ğŸ—ï¸ **arch-agent**      | `deepseek-r1:14b-qwen-distill-q5_K_M` | Architecture & design         | High reasoning depth, DDD, CQRS, scalability tradeâ€‘offs |
| ğŸ§ª **test-agent**      | `qwen2.5-coder:14b-instruct-q5_K_M`   | Testing & QA                  | Generates maintainable tests, detects edge cases        |
| ğŸ—‚ï¸ **plan-agent**      | `qwen2.5:14b-instruct-q4_K_M`         | Specâ€‘driven planning          | Produces detailed specs and milestones                  |
| âš¡ **planâ€‘liteâ€‘agent** | `qwen2.5:7b-instruct-q5_K_M`          | Quick sprint planning         | Lightweight, fast for agile breakdowns                  |
| ğŸ”€ **orch-agent**      | `llama3.1:8b-instruct-q4_0`           | Orchestration                 | CPUâ€‘only router that coordinates all other agents       |

---

## ğŸ’¡ Why Localâ€‘First?

| Benefit              | Description                                                  |
| -------------------- | ------------------------------------------------------------ |
| ğŸ” **Privacy**        | Keep all code and context local â€” ideal for internal projects |
| âš¡ **Performance**    | Quantized models (`q4`, `q5`) fit well in 16â€¯GB VRAM GPUs    |
| ğŸ§© **Specialization** | Each persona is tuned for a distinct dev role                |
| ğŸ§± **Simplicity**     | One Python script â€” no Docker, no cloud                      |
| ğŸ§  **Extensible**     | Add your own personas or change base models easily           |

---

## ğŸ–¥ï¸ Requirements

| Requirement    | Recommended                           |
| -------------- | ------------------------------------- |
| Ollama version | â‰¥ 0.3.8                               |
| GPU            | 16â€¯GB VRAM (RTXâ€¯4060â€¯Ti,â€¯4070,â€¯A2000) |
| RAM            | â‰¥â€¯32â€¯GB                               |
| OS             | Linuxâ€¯(systemd), macOS, or Windowsâ€¯11 |

---

## âš™ï¸ For Windows Users

- Environment variables are applied using `setx` and added to:

  ```
  ~/Documents/PowerShell/Microsoft.PowerShell_profile.ps1
  ```

- Restart PowerShell or log out/in to apply.

---

## ğŸ§© Troubleshooting

| Issue                   | Fix                                                 |
| ----------------------- | --------------------------------------------------- |
| Ollama ignores env vars | Run as `sudo` so env applies to systemd root.       |
| Not enough VRAM         | Use `planâ€‘liteâ€‘agent` or reduce `--num_gpu_layers`. |
| â€œModel not foundâ€       | Run `--pull` before `--create`.                     |
| Ollama service error    | Run `sudo systemctl status ollama` to debug.        |

---

## ğŸ› ï¸ Contributing

Feel free to:

- ğŸ§© Add new personas for specific languages or domains  
- âš™ï¸ Improve model presets or default parameters  
- ğŸ“¦ Submit PRs for crossâ€‘platform optimizations

---

## ğŸ“œ License

Licensed under the **MIT License** â€” free for personal and commercial use.

---

## â¤ï¸ Credits

Built for developers who love:

- ğŸ§  **Ollama**
- âš™ï¸ **Qwenâ€¯2.5**, **DeepSeekâ€¯R1**, **Llamaâ€¯3.1**
- ğŸ’» Clean, private, local AI for everyday coding & architecture

<p align="center"><b>Happy Building ğŸš€</b></p>
