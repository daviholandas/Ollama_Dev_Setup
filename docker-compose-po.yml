version: '3.8'

services:
  po-agent:
    image: nvidia/cuda:12.2.0-runtime-ubuntu22.04
    container_name: vllm-po-agent
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_GPU_MEMORY_UTILIZATION=0.70
      - VLLM_TENSOR_PARALLEL_SIZE=1
      - VLLM_ENABLE_PREFIX_CACHING=1
      - VLLM_MAX_MODEL_LEN=16384
      - VLLM_QUANTIZATION=awq
    volumes:
      - ./models:/models
      - ./outputs:/outputs
      - ./config_po.json:/etc/vllm/config.json
    ports:
      - "8002:8000"  # API
      - "8102:8001"  # Metrics
    entrypoint: >
      bash -c "
        pip install -q vllm[cuda12] &&
        vllm serve Qwen/Qwen2.5-7B-Instruct
          --model-id po-agent
          --gpu-memory-utilization 0.70
          --max-model-len 16384
          --tensor-parallel-size 1
          --enable-prefix-caching
          --quantization awq
          --port 8000
          --served-model-name po
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

networks:
  default:
    name: vllm-network
    driver: bridge
