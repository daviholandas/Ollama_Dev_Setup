version: '3.8'

services:
  dev-agent:
    image: nvidia/cuda:12.2.0-devel-ubuntu22.04
    container_name: vllm-dev-agent
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_GPU_MEMORY_UTILIZATION=0.95
      - VLLM_TENSOR_PARALLEL_SIZE=1
      - VLLM_ENABLE_PREFIX_CACHING=1
      - VLLM_MAX_MODEL_LEN=32768
      - VLLM_QUANTIZATION=gptq
    volumes:
      - ./models:/models
      - ./outputs:/outputs
      - ./config_dev.json:/etc/vllm/config.json
    ports:
      - "8001:8000"  # API
      - "8101:8001"  # Metrics
    entrypoint: >
      bash -c "
        pip install -q vllm[cuda12] &&
        vllm serve Qwen/Qwen2.5-Coder-32B-Instruct
          --model-id dev-agent
          --gpu-memory-utilization 0.95
          --max-model-len 32768
          --tensor-parallel-size 1
          --enable-prefix-caching
          --quantization gptq
          --port 8000
          --served-model-name dev
      "
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

networks:
  default:
    name: vllm-network
    driver: bridge
