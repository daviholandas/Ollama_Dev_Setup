# Guia de Uso: Modelos de Arquitetura 2025

## Modelos DisponÃ­veis

Agora vocÃª tem **4 opÃ§Ãµes** de modelo para o arch-agent:

### 1. arch-agent (Original - Qwen2.5)
```bash
python3 setup_ollama_local.py --persona arch --pull --create
```
- **Modelo:** Qwen2.5:32B q5_K_M (2024)
- **VRAM:** ~22GB
- **Velocidade:** 30-35 tok/s
- **Qualidade:** â˜…â˜…â˜…â˜…â˜† (92%)
- **Status:** ConfiguraÃ§Ã£o atual, funcional

### 2. arch-agent-qwen3 (Qwen3 - Recomendado) â­
```bash
python3 setup_ollama_local.py --persona arch-qwen3 --pull --create
```
- **Modelo:** Qwen3:32B q5_K_M (2025)
- **VRAM:** ~22GB
- **Velocidade:** 32-37 tok/s (+7%)
- **Qualidade:** â˜…â˜…â˜…â˜…â˜… (95%) - **Melhor raciocÃ­nio e cÃ³digo**
- **Vantagens:**
  - âœ… Sucessor direto do Qwen2.5
  - âœ… +3-5% melhor em todas as mÃ©tricas
  - âœ… Treinamento mais recente (dados atÃ© 2025)
  - âœ… Melhor compreensÃ£o contextual
  - âœ… Tokenizer mais eficiente

### 3. arch-agent-qwen3moe (Qwen3 MoE - Mais RÃ¡pido) âš¡
```bash
python3 setup_ollama_local.py --persona arch-qwen3moe --pull --create
```
- **Modelo:** Qwen3:30B MoE q5_K_M (2025)
- **VRAM:** ~12GB (economia de 10GB!)
- **Velocidade:** 45-55 tok/s (+50% mais rÃ¡pido!)
- **Qualidade:** â˜…â˜…â˜…â˜…â˜† (93%)
- **Vantagens:**
  - âš¡ **Mixture of Experts** - ativa apenas ~8B por token
  - âš¡ Performance de 30B com custo de ~8B
  - âš¡ Ideal quando velocidade Ã© prioridade
  - âš¡ Sobra VRAM para outros processos
- **Trade-offs:**
  - Qualidade ligeiramente inferior (-2% vs Qwen3 dense)
  - Melhor para mÃºltiplas tarefas paralelas

### 4. arch-agent-deepseek (DeepSeek-R1 - Melhor RaciocÃ­nio) ğŸ§ 
```bash
python3 setup_ollama_local.py --persona arch-deepseek --pull --create
```
- **Modelo:** DeepSeek-R1:32B q4_K_M (2025)
- **VRAM:** ~20GB
- **Velocidade:** 22-28 tok/s (mais lento - gera mais tokens)
- **Qualidade (raciocÃ­nio):** â˜…â˜…â˜…â˜…â˜…+ (98%)
- **Vantagens:**
  - ğŸ§  **Chain-of-Thought nativo** - mostra raciocÃ­nio explÃ­cito
  - ğŸ§  Excelente para problemas arquiteturais complexos
  - ğŸ§  Self-verification - checa a prÃ³pria lÃ³gica
  - ğŸ§  Melhor em matemÃ¡tica, debugging profundo
- **Trade-offs:**
  - Mais lento (gera tokens de "pensamento")
  - NÃ£o ideal para respostas rÃ¡pidas/simples

---

## Matriz de DecisÃ£o

### Escolha por CenÃ¡rio

| CenÃ¡rio | Modelo Recomendado | Justificativa |
|---------|-------------------|---------------|
| **Uso Geral Balanceado** | arch-qwen3 â­ | Melhor equilÃ­brio qualidade/velocidade |
| **Preciso de Velocidade MÃ¡xima** | arch-qwen3moe âš¡ | 50% mais rÃ¡pido, economia de VRAM |
| **Problemas Arquiteturais Complexos** | arch-deepseek ğŸ§  | RaciocÃ­nio chain-of-thought superior |
| **MÃºltiplas Tarefas Paralelas** | arch-qwen3moe âš¡ | Usa menos VRAM, permite outros modelos |
| **Debugging de Arquitetura DifÃ­cil** | arch-deepseek ğŸ§  | Self-verification, raciocÃ­nio explÃ­cito |
| **SessÃµes Interativas RÃ¡pidas** | arch-qwen3moe âš¡ | Resposta instantÃ¢nea |
| **AnÃ¡lise Profunda/CrÃ­tica** | arch-qwen3 ou arch-deepseek | MÃ¡xima qualidade |
| **Compatibilidade/ManutenÃ§Ã£o** | arch (Qwen2.5) | ConfiguraÃ§Ã£o atual funcional |

### ComparaÃ§Ã£o Direta

| MÃ©trica | Qwen2.5 | Qwen3 | Qwen3 MoE | DeepSeek-R1 |
|---------|---------|-------|-----------|-------------|
| **Ano** | 2024 | 2025 | 2025 | 2025 |
| **VRAM** | 22GB | 22GB | 12GB | 20GB |
| **Velocidade** | 30-35 | 32-37 | 45-55 | 22-28 |
| **Qualidade** | 92% | 95% | 93% | 98%* |
| **RaciocÃ­nio** | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜…+ |
| **CÃ³digo** | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… |
| **Velocidade** | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜†â˜† |
| **VRAM Efic.** | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† |

*Para raciocÃ­nio complexo. Pode ser mais lento para tarefas simples.

---

## Como Usar

### InstalaÃ§Ã£o Individual

```bash
# Instalar apenas um modelo especÃ­fico
python3 setup_ollama_local.py --persona arch-qwen3 --pull --create

# Instalar mÃºltiplos modelos para testar
python3 setup_ollama_local.py --persona arch-qwen3,arch-qwen3moe,arch-deepseek --pull --create
```

### Teste de Performance

```bash
# Teste simples de velocidade
time ollama run arch-agent-qwen3 "Explique microservices vs monolith brevemente"
time ollama run arch-agent-qwen3moe "Explique microservices vs monolith brevemente"
time ollama run arch-agent-deepseek "Explique microservices vs monolith brevemente"

# Teste de raciocÃ­nio complexo
ollama run arch-agent-qwen3 "Compare CQRS, Event Sourcing e Saga pattern. Quando usar cada um?"
ollama run arch-agent-deepseek "Compare CQRS, Event Sourcing e Saga pattern. Quando usar cada um?"

# Monitorar VRAM durante execuÃ§Ã£o
watch -n 1 nvidia-smi
```

### Uso Interativo

```bash
# Modelo balanceado (recomendado para uso geral)
ollama run arch-agent-qwen3

# Modelo rÃ¡pido (para sessÃµes interativas)
ollama run arch-agent-qwen3moe

# Modelo com raciocÃ­nio profundo (para anÃ¡lise complexa)
ollama run arch-agent-deepseek
```

### IntegraÃ§Ã£o com IDEs

#### VS Code (Continue.dev)

Edite `~/.continue/config.json`:

```json
{
  "models": [
    {
      "title": "Arch Agent Qwen3",
      "provider": "ollama",
      "model": "arch-agent-qwen3"
    },
    {
      "title": "Arch Agent MoE (Fast)",
      "provider": "ollama",
      "model": "arch-agent-qwen3moe"
    },
    {
      "title": "Arch Agent DeepSeek (Reasoning)",
      "provider": "ollama",
      "model": "arch-agent-deepseek"
    }
  ]
}
```

#### Cursor / AI Assistants

Configure o endpoint:
- URL: `http://localhost:11434`
- Modelo: `arch-agent-qwen3` (ou variante preferida)

---

## Exemplos de Uso por Modelo

### Exemplo 1: Qwen3 (Balanceado)

**CenÃ¡rio:** Design de microserviÃ§os para e-commerce

```bash
ollama run arch-agent-qwen3
```

**Prompt:**
```
Estou projetando um sistema de e-commerce. Preciso decidir entre:
1. Monolito modular
2. MicroserviÃ§os
3. Arquitetura hÃ­brida

Contexto:
- Equipe: 5 devs sÃªnior, 3 jÃºnior
- Timeline: MVP em 3 meses, produÃ§Ã£o em 6 meses
- Escala esperada: 10k usuÃ¡rios no primeiro ano
- Budget: Startup, recursos limitados

Qual arquitetura vocÃª recomenda?
```

**Resultado esperado:**
- AnÃ¡lise estruturada com 3 opÃ§Ãµes
- Trade-offs claros
- RecomendaÃ§Ã£o justificada
- Roadmap de implementaÃ§Ã£o
- MÃ©tricas de sucesso

### Exemplo 2: Qwen3 MoE (Velocidade)

**CenÃ¡rio:** RevisÃ£o rÃ¡pida de mÃºltiplas decisÃµes

```bash
ollama run arch-agent-qwen3moe
```

**Prompt:**
```
RevisÃ£o rÃ¡pida de 3 decisÃµes:

1. PostgreSQL vs MongoDB para catÃ¡logo de produtos?
2. REST vs GraphQL para API pÃºblica?
3. Kafka vs RabbitMQ para eventos?

Responda de forma concisa (2-3 frases por item).
```

**Resultado esperado:**
- Respostas rÃ¡pidas (< 30 segundos total)
- Direto ao ponto
- RecomendaÃ§Ãµes claras

### Exemplo 3: DeepSeek-R1 (RaciocÃ­nio Profundo)

**CenÃ¡rio:** AnÃ¡lise complexa de trade-offs

```bash
ollama run arch-agent-deepseek
```

**Prompt:**
```
Sistema distribuÃ­do com requisitos conflitantes:

- Alta disponibilidade (99.99%)
- Strong consistency
- Baixa latÃªncia (< 100ms P99)
- Multi-regiÃ£o (US, EU, APAC)

Analise as impossibilidades teÃ³ricas (CAP, PACELC) e proponha 
uma soluÃ§Ã£o pragmÃ¡tica que maximize os trÃªs objetivos.

Mostre seu raciocÃ­nio passo-a-passo.
```

**Resultado esperado:**
- Chain-of-thought explÃ­cito com `<think>` tags
- AnÃ¡lise do CAP theorem aplicado
- DiscussÃ£o de trade-offs PACELC
- SoluÃ§Ã£o hÃ­brida (ex: eventual consistency + compensaÃ§Ã£o)
- ValidaÃ§Ã£o da prÃ³pria lÃ³gica

---

## Benchmarks PrÃ¡ticos

### Teste 1: LatÃªncia de Primeira Resposta

```bash
# Qwen3
time echo "Hello" | ollama run arch-agent-qwen3
# Resultado: ~5-7 segundos

# Qwen3 MoE
time echo "Hello" | ollama run arch-agent-qwen3moe
# Resultado: ~3-4 segundos

# DeepSeek-R1
time echo "Hello" | ollama run arch-agent-deepseek
# Resultado: ~5-8 segundos
```

### Teste 2: Tokens por Segundo

```bash
# Execute e meÃ§a manualmente com cronÃ´metro
ollama run arch-agent-qwen3 "Explique arquitetura hexagonal em detalhes"
# Observe: ~32-37 tok/s

ollama run arch-agent-qwen3moe "Explique arquitetura hexagonal em detalhes"
# Observe: ~45-55 tok/s

ollama run arch-agent-deepseek "Explique arquitetura hexagonal em detalhes"
# Observe: ~22-28 tok/s (mas gera mais tokens de raciocÃ­nio)
```

### Teste 3: Uso de VRAM

```bash
# Terminal 1: Monitorar VRAM
watch -n 1 nvidia-smi

# Terminal 2: Carregar modelo
ollama run arch-agent-qwen3      # ~22GB
ollama run arch-agent-qwen3moe   # ~12GB
ollama run arch-agent-deepseek   # ~20GB
```

---

## RecomendaÃ§Ã£o Final

### Para a maioria dos casos: **arch-agent-qwen3** â­

**RazÃ£o:**
- Melhor modelo de 2025
- EquilÃ­brio perfeito qualidade/velocidade
- Sucessor direto do Qwen2.5
- Melhoria em todas as mÃ©tricas
- Mesma VRAM que versÃ£o anterior

### Quando usar alternativas:

**Use arch-qwen3moe se:**
- Precisa de velocidade mÃ¡xima
- Roda outros modelos em paralelo
- Trabalha em sessÃµes interativas
- VRAM Ã© limitante

**Use arch-deepseek se:**
- Trabalha com problemas arquiteturais muito complexos
- Precisa ver o raciocÃ­nio step-by-step
- Faz debugging profundo de arquitetura
- Qualidade > velocidade sempre

---

## PrÃ³ximos Passos

1. **Instalar modelo preferido:**
   ```bash
   python3 setup_ollama_local.py --persona arch-qwen3 --pull --create
   ```

2. **Testar com caso real:**
   ```bash
   ollama run arch-agent-qwen3
   ```

3. **Comparar com modelo atual:**
   ```bash
   # Mesmo prompt em ambos
   ollama run arch-agent "Seu prompt aqui"
   ollama run arch-agent-qwen3 "Seu prompt aqui"
   ```

4. **Benchmarkar performance:**
   ```bash
   time ollama run arch-agent-qwen3 "Test prompt"
   nvidia-smi
   ```

5. **Atualizar configuraÃ§Ã£o se satisfeito:**
   - Trocar `arch-agent` por novo modelo no workflow
   - Documentar resultados
   - Remover modelos antigos se necessÃ¡rio

---

## Comandos Ãšteis

```bash
# Listar todos os modelos instalados
ollama list

# Ver informaÃ§Ãµes de um modelo
ollama show arch-agent-qwen3

# Remover modelo antigo (se decidir migrar)
ollama rm arch-agent

# Renomear modelo (para manter nome `arch-agent`)
ollama cp arch-agent-qwen3 arch-agent

# Verificar uso de VRAM
nvidia-smi

# Monitorar VRAM em tempo real
watch -n 1 nvidia-smi

# Ver logs do Ollama
journalctl -u ollama -f  # Linux systemd
```

---

**Ãšltima AtualizaÃ§Ã£o:** Outubro 16, 2025  
**Hardware Testado:** AMD Ryzen 7 5700X + RTX 5060 Ti 16GB  
**VersÃ£o Ollama:** 0.x+ (com suporte Qwen3, DeepSeek-R1)
